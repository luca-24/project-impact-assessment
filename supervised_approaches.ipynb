{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_docs = True\n",
    "dump_docs = False\n",
    "language='en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, random, itertools, pandas\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import spacy\n",
    "from spacy.pipeline import TextCategorizer\n",
    "from spacy.tokens import Doc, Span\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import coverage_error, label_ranking_average_precision_score, label_ranking_loss\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run evaluation.ipynb\n",
    "%run utils.ipynb\n",
    "%run nlp_functions.ipynb\n",
    "%run explanation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/rtatman/data-cleaning-challenge-scale-and-normalize-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(project_docs, goal_docs, feature_type='goal_similarities', label_format='sparse', scale=True, shuffle=False):\n",
    "    assert feature_type in ('goal_similarities', 'embeddings')\n",
    "    assert label_format in ('sparse', 'dense')\n",
    "    \n",
    "    feature_vectors = []\n",
    "    labels = []\n",
    "    for pdoc in project_docs:\n",
    "        if label_format == 'sparse':\n",
    "            true_labels = [1 if i in pdoc._.goal_labels else 0 for i in range(14)]\n",
    "        else:\n",
    "            true_labels = pdoc._.goal_labels\n",
    "            \n",
    "        if feature_type == 'goal_similarities':\n",
    "            features = compute_goal_scores(pdoc, goal_docs, similarity='custom')\n",
    "        elif feature_type == 'embeddings':\n",
    "            features = pdoc._.custom_vector\n",
    "                    \n",
    "        labels.append(true_labels)\n",
    "        feature_vectors.append(features)\n",
    "    \n",
    "    feature_vectors, labels = np.array(feature_vectors), np.array(labels)\n",
    "    \n",
    "    if scale:\n",
    "        feature_vectors = RobustScaler().fit_transform(feature_vectors)\n",
    "        \n",
    "    if shuffle:\n",
    "        zipped = list(zip(feature_vectors, labels))\n",
    "        random.seed(10)\n",
    "        random.shuffle(zipped)\n",
    "        feature_vectors, labels = zip(*zipped)\n",
    "        feature_vectors, labels = np.array(feature_vectors), np.array(labels)\n",
    "    \n",
    "    return feature_vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(label_matrix):\n",
    "    \n",
    "    counts = {i:0 for i in range(label_matrix.shape[1])}\n",
    "    for label_vector in label_matrix:\n",
    "        for index,l in enumerate(label_vector):\n",
    "            if l:\n",
    "                counts[index] += 1\n",
    "    class_weights_dict = {i: 1.0 / counts[i] for i in counts}\n",
    "    \n",
    "    #for i in counts:\n",
    "    #    print(i, counts[i], class_weights_dict[i])\n",
    "    return class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_weak_labels(training_project_docs, unlabeled_project_docs, goal_docs, algorithm,\n",
    "                       #next line is the parameters to optimize\n",
    "                       batch_size=100, label_selection_method='dgc', dcg_min_diff=0.1, threshold=0.3, k=3,\n",
    "                       feature_type='goal_similarities', scale_features=True, parameters={}, \n",
    "                       plot_epochs=True, verbose=True):\n",
    "    assert label_selection_method in ('dcg', 'threshold', 'k')\n",
    "    if label_selection_method == 'dcg':\n",
    "        assert dcg_min_diff\n",
    "    elif label_selection_method == 'threshold':\n",
    "        assert threshold\n",
    "    elif label_selection_method == 'k':\n",
    "        assert k\n",
    "    \n",
    "    n_iter = 0\n",
    "    while len(unlabeled_project_docs) > 0:\n",
    "        print('\\nTraining set size:', len(training_project_docs))\n",
    "        print('Unlabeled set size:', len(unlabeled_project_docs))\n",
    "        \n",
    "        parameters['random_state'] = n_iter\n",
    "        n_iter += 1\n",
    "        offset = min(batch_size, len(unlabeled_project_docs))\n",
    "        unlabeled_batch = unlabeled_project_docs[:offset]\n",
    "        _, u_score = test_classifier(training_project_docs, unlabeled_batch, goal_docs, algorithm=algorithm,\n",
    "                                     feature_type=feature_type, scale_features=scale_features, parameters=parameters, \n",
    "                                     plot_epochs=plot_epochs, shuffle=True, verbose=verbose)\n",
    "        \n",
    "        for udoc, us in zip(unlabeled_batch, u_score):\n",
    "            if label_selection_method == 'dcg':\n",
    "                assigned_labels = select_labels_by_dcg(us, min_diff=dcg_min_diff)\n",
    "            elif label_selection_method == 'threshold':\n",
    "                assigned_labels = select_labels_by_threshold(us, threshold=threshold)\n",
    "            elif label_selection_method == 'k':\n",
    "                assigned_labels = select_labels_by_k(us, k=k)\n",
    "            udoc._.goal_labels = assigned_labels\n",
    "            \n",
    "        unlabeled_batch = [udoc for udoc in unlabeled_batch if len(udoc._.goal_labels) > 0]\n",
    "        training_project_docs = training_project_docs + unlabeled_batch\n",
    "        unlabeled_project_docs = unlabeled_project_docs[offset:]\n",
    "    \n",
    "    print('\\nTraining set size:', len(training_project_docs))\n",
    "    print('Unlabeled set size:', len(unlabeled_project_docs))\n",
    "    return training_project_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_spacy(training_project_docs, test_project_docs, parameters={}, load_model=False, verbose=True):\n",
    "    for p in parameters:\n",
    "        assert p in ['n_iter', 'batch_size']\n",
    "        \n",
    "    if verbose:\n",
    "        print('Algorithm: spacy')\n",
    "    \n",
    "    #nlp = en_trf_bertbaseuncased_lg.load()\n",
    "    nlp=spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    if load_model:\n",
    "        print('Loading the model...')\n",
    "        text_classifier = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": False, \"architecture\": \"ensemble\"})\n",
    "        text_classifier.from_disk('models/spacy_textcategorizer_sm')\n",
    "        nlp.add_pipe(text_classifier, last=True)\n",
    "        \n",
    "    else:\n",
    "        n_iter = parameters['n_iter'] if 'n_iter' in parameters else 10\n",
    "        batch_size = parameters['batch_size'] if 'batch_size' in parameters else 8\n",
    "\n",
    "        if verbose:\n",
    "            print('Preparing nlp model...')\n",
    "        text_classifier = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": False, \"architecture\": \"ensemble\"})\n",
    "        nlp.add_pipe(text_classifier, last=True)\n",
    "        if verbose:\n",
    "            print('Pipeline:', nlp.pipe_names)\n",
    "\n",
    "        for i in range(14):\n",
    "            text_classifier.add_label(str(i))\n",
    "\n",
    "        if verbose:\n",
    "            print('Preparing samples...')\n",
    "\n",
    "        training_samples = [(pdoc.text, [str(l) for l in pdoc._.goal_labels]) for pdoc in training_project_docs]\n",
    "        training_samples = [(text, {'cats': {label: (label in true_labels) for label in text_classifier.labels}}) \n",
    "                            for text,true_labels in training_samples]\n",
    "\n",
    "        #training_samples = training_samples[:50]\n",
    "\n",
    "        spacy.util.fix_random_seed()\n",
    "        other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "        with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "            nlp.begin_training()\n",
    "            if verbose:\n",
    "                print('Starting training...')\n",
    "            for itn in range(n_iter):\n",
    "                if verbose:\n",
    "                    print('Training loop: ', itn)\n",
    "                random.shuffle(training_samples)\n",
    "                # Divide examples into batches\n",
    "                for batch in spacy.util.minibatch(training_samples, size=batch_size):\n",
    "                    texts = [text for text, label in batch]\n",
    "                    labels = [label for text, label in batch]\n",
    "                    #print('Updating...')\n",
    "                    # Update the model\n",
    "                    nlp.update(docs=texts, golds=labels)\n",
    "\n",
    "        text_classifier = nlp.get_pipe('textcat')\n",
    "        print('Saving the model...')\n",
    "        text_classifier.to_disk('models/spacy_textcategorizer_sm')\n",
    "    \n",
    "    y_score, tensors = text_classifier.predict(test_project_docs)\n",
    "    for test_pdoc,ps in zip(test_project_docs,y_score):\n",
    "        test_pdoc._.predicted_goal_scores = sorted([(i,s) for i,s in enumerate(list(ps))], reverse=True, key=lambda x:x[-1])\n",
    "\n",
    "    y_true = [[1 if int(l) in test_pdoc._.goal_labels else 0 for l in text_classifier.labels] \n",
    "              for test_pdoc in test_project_docs]\n",
    "    \n",
    "    return y_true, y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_keras_epochs(history):\n",
    "    \n",
    "    fig, axs = plt.subplots(2, figsize = (8,6))\n",
    "    axs[0].plot(history.history['loss'], label='training')\n",
    "    axs[0].plot(history.history['val_loss'], label='validation')\n",
    "    axs[0].set_title('Loss')\n",
    "    axs[0].set(xlabel = 'Epochs')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history.history['categorical_accuracy'], label='training')\n",
    "    axs[1].plot(history.history['val_categorical_accuracy'], label='validation')\n",
    "    axs[1].set_title('categorical_accuracy')\n",
    "    axs[1].set(xlabel = 'Epochs')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_keras_model(parameters):\n",
    "        \n",
    "    tf.random.set_seed(0)\n",
    "    \n",
    "    training_vectors_shape = parameters['training_vectors_shape']\n",
    "    n_labels = parameters['n_labels']\n",
    "    layer_type = parameters['layer_type'] if 'layer_type' in parameters else 'Dense'\n",
    "    hidden_layer_sizes = parameters['hidden_layer_sizes'] if 'hidden_layer_sizes' in parameters else (100,)\n",
    "    dropout = parameters['dropout'] if 'dropout' in parameters else 0.0\n",
    "    learning_rate = parameters['learning_rate'] if 'learning_rate' in parameters else 0.001\n",
    "    print_summary = parameters['print_summary'] if 'print_summary' in parameters else False\n",
    "    \n",
    "    model = Sequential()\n",
    "    for n_neurons in hidden_layer_sizes:\n",
    "        model.add(Dense(n_neurons, input_dim=training_vectors_shape[1], activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "            \n",
    "    model.add(Dense(n_labels, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=Adam(lr=learning_rate), \n",
    "                  metrics=['categorical_accuracy'])\n",
    "    \n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_sklearn_wrapper(parameters):\n",
    "    \n",
    "    assert 'training_vectors_shape' in parameters\n",
    "    assert 'n_labels' in parameters\n",
    "    if 'layer_type' in parameters:\n",
    "        assert parameters['layer_type'] in ('Dense')\n",
    "    for p_name in parameters:\n",
    "        assert p_name in ('layer_type', 'hidden_layer_sizes', 'training_vectors_shape', 'n_labels', 'dropout', 'learning_rate',\n",
    "                          'n_epochs', 'batch_size', 'class_weights_dict', 'early_stopping', 'patience', 'random_state')\n",
    "        \n",
    "    n_epochs = parameters['n_epochs'] if 'n_epochs' in parameters else 500\n",
    "    batch_size = parameters['batch_size'] if 'batch_size' in parameters else 64\n",
    "    early_stopping = parameters['early_stopping'] if 'early_stopping' in parameters else False\n",
    "    patience = parameters['patience'] if 'patience' in parameters else 10\n",
    "    class_weights_dict = parameters['class_weights_dict'] if 'class_weights_dict' in parameters else None\n",
    "    random_state = parameters['random_state'] if 'random_state' in parameters else 0\n",
    "    \n",
    "    callbacks = [EarlyStopping(patience=patience)] if early_stopping else []    \n",
    "    classifier = KerasClassifier(build_fn=lambda:compile_keras_model(parameters),\n",
    "                                 epochs=n_epochs,\n",
    "                                 batch_size=batch_size,\n",
    "                                 callbacks=callbacks,\n",
    "                                 validation_split=0.2,\n",
    "                                 class_weight=class_weights_dict,\n",
    "                                 random_state=random_state,\n",
    "                                 verbose=False)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_classifier(algorithm, parameters, verbose=True):\n",
    "    assert algorithm in ('RandomForest', 'GradientBoosting', 'SVM', 'MLP', 'keras')\n",
    "    \n",
    "    if algorithm == 'RandomForest':\n",
    "        for p_name in parameters:\n",
    "            assert p_name in ('n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'random_state')\n",
    "        if 'n_estimators' not in parameters:\n",
    "            parameters['n_estimators'] = 100\n",
    "        if 'max_depth' not in parameters:\n",
    "            parameters['max_depth'] = None\n",
    "        if 'min_samples_split' not in parameters:\n",
    "            parameters['min_samples_split'] = 2\n",
    "        if 'min_samples_leaf' not in parameters:\n",
    "            parameters['min_samples_leaf'] = 1\n",
    "        if 'random_state' not in parameters:\n",
    "            parameters['random_state'] = 0\n",
    "        base_classifier = RandomForestClassifier(n_estimators=parameters['n_estimators'],\n",
    "                                                 max_depth=parameters['max_depth'],\n",
    "                                                 min_samples_split=parameters['min_samples_split'],\n",
    "                                                 min_samples_leaf=parameters['min_samples_leaf'],\n",
    "                                                 random_state=parameters['random_state'],\n",
    "                                                 class_weight='balanced',\n",
    "                                                 n_jobs=-1)\n",
    "    elif algorithm == 'GradientBoosting':\n",
    "        for p_name in parameters:\n",
    "            assert p_name in ('n_estimators', 'max_depth', 'min_samples_split', \n",
    "                              'min_samples_leaf', 'learning_rate', 'random_state')\n",
    "        if 'n_estimators' not in parameters:\n",
    "            parameters['n_estimators'] = 100\n",
    "        if 'learning_rate' not in parameters:\n",
    "            parameters['learning_rate'] = 0.1\n",
    "        if 'min_samples_split' not in parameters:\n",
    "            parameters['min_samples_split'] = 2\n",
    "        if 'min_samples_leaf' not in parameters:\n",
    "            parameters['min_samples_leaf'] = 1\n",
    "        if 'max_depth' not in parameters:\n",
    "            parameters['max_depth'] = 3\n",
    "        if 'random_state' not in parameters:\n",
    "            parameters['random_state'] = 0\n",
    "        base_classifier = GradientBoostingClassifier(n_estimators=parameters['n_estimators'],\n",
    "                                                     learning_rate=parameters['learning_rate'],\n",
    "                                                     min_samples_split=parameters['min_samples_split'],\n",
    "                                                     min_samples_leaf=parameters['min_samples_leaf'],\n",
    "                                                     max_depth=parameters['max_depth'],        \n",
    "                                                     random_state=parameters['random_state'])\n",
    "    elif algorithm == 'SVM':\n",
    "        for p_name in parameters:\n",
    "            assert p_name in ('C', 'kernel', 'gamma', 'random_state')\n",
    "        if 'C' not in parameters:\n",
    "            parameters['C'] = 1.0\n",
    "        if 'kernel' not in parameters:\n",
    "            parameters['kernel'] = 'rbf'\n",
    "        if 'gamma' not in parameters: #Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’\n",
    "            parameters['gamma'] = 'scale'\n",
    "        if 'random_state' not in parameters:\n",
    "            parameters['random_state'] = 0\n",
    "        base_classifier = SVC(C=parameters['C'],\n",
    "                              kernel=parameters['kernel'],\n",
    "                              gamma=parameters['gamma'],\n",
    "                              class_weight='balanced',\n",
    "                              probability=True, \n",
    "                              random_state=parameters['random_state'])\n",
    "    \n",
    "    elif algorithm == 'MLP':\n",
    "        for p_name in parameters:\n",
    "            assert p_name in ('hidden_layer_sizes', 'alpha', 'batch_size', 'max_iter','tol', 'random_state')\n",
    "        if 'hidden_layer_sizes' not in parameters:\n",
    "            parameters['hidden_layer_sizes'] = (100,)\n",
    "        if 'alpha' not in parameters:\n",
    "            parameters['alpha'] = 0.0001\n",
    "        if 'batch_size' not in parameters:\n",
    "            parameters['batch_size'] = 'auto'\n",
    "        if 'max_iter' not in parameters:\n",
    "            parameters['max_iter'] = 500\n",
    "        if 'tol' not in parameters:\n",
    "            parameters['tol'] = 0.0001\n",
    "        if 'random_state' not in parameters:\n",
    "            parameters['random_state'] = 0\n",
    "        base_classifier = MLPClassifier(hidden_layer_sizes=parameters['hidden_layer_sizes'],\n",
    "                                        alpha=parameters['alpha'],\n",
    "                                        batch_size=parameters['batch_size'],\n",
    "                                        max_iter=parameters['max_iter'],\n",
    "                                        tol=parameters['tol'],\n",
    "                                        validation_fraction=0.2,\n",
    "                                        random_state=parameters['random_state'])\n",
    "    elif algorithm == 'keras':\n",
    "        base_classifier = get_keras_sklearn_wrapper(parameters)  \n",
    "        parameters = base_classifier.get_params()\n",
    "        \n",
    "    if verbose:\n",
    "        print('\\nBase estimator:', algorithm)\n",
    "        if algorithm != 'keras':\n",
    "            print('Meta estimator: One vs all')\n",
    "        print('Parameters:')\n",
    "        for p in parameters:\n",
    "            print(p, ':', parameters[p])\n",
    "            \n",
    "    if algorithm == 'keras':\n",
    "        return base_classifier\n",
    "    else:\n",
    "        return OneVsRestClassifier(base_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(feature_vectors, labels, algorithm='RandomForest', n_folds=10, parameters={}):\n",
    "    \n",
    "    if algorithm == 'keras':\n",
    "        parameters['training_vectors_shape'] = feature_vectors.shape\n",
    "        parameters['n_labels'] = 14\n",
    "        parameters['class_weights_dict'] = compute_class_weights(labels)\n",
    "\n",
    "    classifier = compile_classifier(algorithm=algorithm, parameters=parameters, verbose=False)\n",
    "    \n",
    "    y_true = []\n",
    "    y_score = []\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "    for training_index, validation_index in kf.split(feature_vectors):\n",
    "        training_vectors, validation_vectors = feature_vectors[training_index], feature_vectors[validation_index]\n",
    "        training_labels, validation_labels = labels[training_index], labels[validation_index]\n",
    "\n",
    "        classifier.fit(training_vectors, training_labels)\n",
    "        predicted_scores = classifier.predict_proba(validation_vectors)\n",
    "        \n",
    "        for vindex,ps in zip(validation_index,predicted_scores):\n",
    "            project_docs[vindex]._.predicted_goal_scores = sorted([(i,s) for i,s in enumerate(list(ps))], reverse=True, key=lambda x:x[-1])\n",
    "\n",
    "        y_true.extend(list(validation_labels))\n",
    "        y_score.extend(list(predicted_scores))\n",
    "    \n",
    "    return y_true, y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_classifier(project_docs, goal_docs, algorithm, parameter_ranges, feature_type='goal_similarities',\n",
    "                        scale_features=True, n_folds=10, metric='lrap', shuffle=False, verbose=True):\n",
    "    assert metric in ('lrap', 'lrl', 'cov_err')\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\nN. folds:', n_folds)\n",
    "        print('Scale features:', scale_features)\n",
    "        print('Metric:', metric)\n",
    "    \n",
    "    feature_vectors, labels = prepare_data(project_docs, goal_docs, feature_type=feature_type, \n",
    "                                           label_format='sparse', scale=scale_features, shuffle=shuffle)\n",
    "    \n",
    "    param_lists = []\n",
    "    for p_name in parameter_ranges:\n",
    "        l = [(p_name, p_value) for p_value in parameter_ranges[p_name]]\n",
    "        param_lists.append(l)\n",
    "        \n",
    "    param_configurations = []\n",
    "    for combination in itertools.product(*param_lists):\n",
    "        param_dict = {}\n",
    "        for p_name, p_value in combination:\n",
    "            param_dict[p_name] = p_value\n",
    "        param_configurations.append(param_dict)\n",
    "        \n",
    "    original_parameters = list(param_configurations[0].keys()) + ['score']\n",
    "    \n",
    "    print('\\nStarting parameter optimization...')\n",
    "    iteration = 0\n",
    "    for pconf in param_configurations:\n",
    "        #if (iteration % 10 == 0):\n",
    "            #print(int(100*(iteration / len(param_configurations))), '%')\n",
    "        print(pconf)\n",
    "        iteration += 1\n",
    "        y_true, y_score = cross_validate(feature_vectors, labels,\n",
    "                                         algorithm=algorithm, parameters=pconf,\n",
    "                                         n_folds=n_folds)\n",
    "        \n",
    "        if metric == 'lrap':\n",
    "            score = label_ranking_average_precision_score(y_true, y_score)\n",
    "        elif metric == 'lrl':\n",
    "            score = label_ranking_loss(y_true, y_score)\n",
    "        else:\n",
    "            score = coverage_error(y_true, y_score)\n",
    "        pconf['score'] = score\n",
    "    \n",
    "    print('Done.')\n",
    "    grid = pandas.DataFrame(data=param_configurations)\n",
    "    grid = grid[original_parameters]\n",
    "    grid = grid.sort_values(by=['score'], ascending=True if metric in ('lrl','cov_err') else False)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(training_project_docs, test_project_docs, goal_docs, algorithm,\n",
    "                    feature_type='goal_similarities', scale_features=True, parameters={}, \n",
    "                    plot_epochs=True, shuffle=False, load_model=False, verbose=True):\n",
    "    \n",
    "    if algorithm == 'spacy':\n",
    "        return classify_with_spacy(training_project_docs, test_project_docs, parameters=parameters, \n",
    "                                   load_model=load_model, verbose=verbose)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\nScale features:', scale_features)\n",
    "        \n",
    "    # siamo totalmente fair, perchè lo scaling/normalizzazione viene fatto distintamente per\n",
    "    # training set e test set\n",
    "    feature_vectors_train, labels_train = prepare_data(training_project_docs, goal_docs, feature_type=feature_type, \n",
    "                                                       label_format='sparse', scale=scale_features, shuffle=shuffle)\n",
    "    feature_vectors_test, labels_test = prepare_data(test_project_docs, goal_docs, feature_type=feature_type, \n",
    "                                                     label_format='sparse', scale=scale_features, shuffle=shuffle)\n",
    "        \n",
    "    if algorithm == 'keras':\n",
    "        parameters['training_vectors_shape'] = feature_vectors_train.shape\n",
    "        parameters['n_labels'] = 14\n",
    "        parameters['class_weights_dict'] = compute_class_weights(labels_train)\n",
    "        \n",
    "    classifier = compile_classifier(algorithm=algorithm, parameters=parameters, verbose=verbose)\n",
    "            \n",
    "    if verbose:\n",
    "        print('\\nFitting classifier...')\n",
    "    history = classifier.fit(feature_vectors_train, labels_train)\n",
    "    \n",
    "    if algorithm == 'keras' and plot_epochs:\n",
    "        plot_keras_epochs(history)\n",
    "        \n",
    "    if verbose:\n",
    "        print('Predicting test labels...')\n",
    "    predicted_scores = classifier.predict_proba(feature_vectors_test)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Done.')\n",
    "    \n",
    "    for test_pdoc,ps in zip(test_project_docs,predicted_scores):\n",
    "        test_pdoc._.predicted_goal_scores = sorted([(i,s) for i,s in enumerate(list(ps))], reverse=True, key=lambda x:x[-1])\n",
    "        \n",
    "    y_true = list(labels_test)\n",
    "    y_score = list(predicted_scores)\n",
    "    return y_true, y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_spacy_extensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_docs:\n",
    "    with open('data/pickles/project_docs_labeled_optimized_'+language+'.pkl', 'rb') as f:\n",
    "        project_docs = pickle.load(f)\n",
    "    with open('data/pickles/project_docs_unlabeled_optimized_'+language+'.pkl', 'rb') as f:\n",
    "        unlabeled_project_docs = pickle.load(f)\n",
    "    with open('data/pickles/goal_docs_optimized_'+language+'.pkl', 'rb') as f:\n",
    "        goal_docs = pickle.load(f)\n",
    "else:\n",
    "    projects_df = pandas.read_csv('data/ris3-mcat-projects-cleaned-'+language+'.csv', sep='\\t')\n",
    "    goals_df = pandas.read_excel('data/un-goals.xlsx')\n",
    "        \n",
    "    project_docs, unlabeled_project_docs, goal_docs = generate_project_and_goal_docs(projects_df, goals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dump_docs:\n",
    "    with open('data/pickles/project_docs_labeled_optimized_'+language+'.pkl', 'wb') as f:\n",
    "        pickle.dump(project_docs, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data/pickles/project_docs_unlabeled_optimized_'+language+'.pkl', 'wb') as f:\n",
    "        pickle.dump(unlabeled_project_docs, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('data/pickles/goal_docs_optimized_'+language+'.pkl', 'wb') as f:\n",
    "        pickle.dump(goal_docs, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences = np.zeros(14)\n",
    "for pdoc in project_docs:\n",
    "    for l in pdoc._.goal_labels:\n",
    "        occurrences[l] += 1\n",
    "\n",
    "label_names = list(goal_name_mapping.values())\n",
    "occurrences, label_names = zip(*sorted([(o,l) for o,l in zip(occurrences, label_names)], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=12)    # fontsize of the tick labels\n",
    "x = np.arange(len(occurrences))\n",
    "fig, ax = plt.subplots(figsize = (12, 3))\n",
    "ax.bar(x, occurrences)\n",
    "ax.set_ylabel('occurrences')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(label_names, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ranges = {'hidden_layer_sizes': [(100,), (100,50)], \n",
    "            'n_epochs': [5, 10]}\n",
    "grid = optimize_classifier(training_project_docs, goal_docs, algorithm='keras', \n",
    "                           parameter_ranges=p_ranges, scale_features=True,\n",
    "                           n_folds=3, metric='lrap', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.to_excel('results/optimization.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "test_project_docs = random.sample(project_docs, 10)\n",
    "training_project_docs = [pdoc for pdoc in project_docs if pdoc not in test_project_docs]\n",
    "\n",
    "print(len(training_project_docs))\n",
    "print(len(test_project_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_training = []\n",
    "for pdoc in training_project_docs:\n",
    "    yt = [1 if i in pdoc._.goal_labels else 0 for i in range(14)]\n",
    "    y_true_training.append(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frequencies = calculate_label_frequencies(y_true_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={'C':512, 'gamma':0.0009765625}\n",
    "y_true_test, y_score_test = test_classifier(extended_training_project_docs, test_project_docs, goal_docs,\n",
    "                                            feature_type='goal_similarities', scale_features=True,\n",
    "                                            algorithm='SVM', parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run evaluation.ipynb\n",
    "ranking_metrics = compute_ranking_metrics(y_true_test, y_score_test)\n",
    "\n",
    "classification_metrics = compute_classification_metrics(y_true_test, y_score_test, \n",
    "                                                                   label_selection_method='threshold',\n",
    "                                                                   threshold=0.6)\n",
    "\n",
    "classification_metrics_per_class = compute_binary_classification_metrics_per_class(y_true_test, y_score_test, \n",
    "                                                                                   label_selection_method='threshold',\n",
    "                                                                                   threshold=0.5)\n",
    "\n",
    "print('RESULTS ON TEST SET')\n",
    "print('\\nRanking metrics:')\n",
    "print_metrics(ranking_metrics)\n",
    "print('\\nClassification metrics:')\n",
    "print_metrics(classification_metrics)\n",
    "print('\\nClassification metrics per class:')\n",
    "for l in classification_metrics_per_class:\n",
    "    print('\\nLabel:', l)\n",
    "    print_metrics(classification_metrics_per_class[l])\n",
    "    \n",
    "bias_metrics = compute_imbalance_bias_metrics(classification_metrics_per_class, training_frequencies)\n",
    "\n",
    "print('\\nImbalance Bias metrics:')\n",
    "print_metrics(bias_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_output(test_project_docs, goal_docs, percentile_highlighted_words=75, use_colors=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_training_project_docs = assign_weak_labels(training_project_docs, unlabeled_project_docs, goal_docs, \n",
    "                                                    algorithm='RandomForest',\n",
    "                                                    parameters={'max_depth':None, 'min_samples_leaf':1,\n",
    "                                                                'min_samples_split':4, 'n_estimators':200},\n",
    "                                                    batch_size=25, label_selection_method='threshold', threshold=0.5,\n",
    "                                                    feature_type='goal_similarities', scale_features=True,  \n",
    "                                                    verbose=False)\n",
    "\n",
    "with open('data/pickles/extended_training_project_docs_RF_threshold_0.5.pkl', 'wb') as f:\n",
    "    pickle.dump(extended_training_project_docs, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
