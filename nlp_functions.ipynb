{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode, re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "import spacy, en_trf_bertbaseuncased_lg\n",
    "from gensim.parsing import preprocessing as pproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stopwords-ca.txt', 'r') as f:\n",
    "    stopwords_ca = [unidecode.unidecode(word.strip()) for word in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply standard pre-processing techniques to a text and return the normalized string.\n",
    "\"\"\"\n",
    "def process_text(string, remove_stopwords=True, stemming=False, language='en'):\n",
    "    \n",
    "    if language == 'ca' and stemming:\n",
    "        print('Warning: cannot perform stemming on catalan.')\n",
    "        \n",
    "    string = unidecode.unidecode(string)\n",
    "    string = string.lower()\n",
    "    abbreviations = re.findall(r'(?:[a-z]\\.)+', string)\n",
    "    for abbr in abbreviations:\n",
    "        string = string.replace(abbr, abbr.replace('.',''))\n",
    "    string = pproc.strip_punctuation2(string)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        if language == 'en':\n",
    "            string = pproc.remove_stopwords(string)\n",
    "        else:\n",
    "            string = ' '.join([t for t in string.split() if t not in stopwords_ca and len(t)>1])\n",
    "    \n",
    "    if stemming and language == 'en':\n",
    "        string = pproc.stem_text(string)\n",
    "        \n",
    "    string = string.strip()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a collection of spacy documents, estimate the relevance of each word inside each document by means of TF-IDF. Words that appear in more than max_df (in percentage) documents or in less than min_df (in absolute value) documents are filtered out.\n",
    "\n",
    "The word relevances are stored in an extended attribute of the docs called \"word_relevances\", as a list of tuples (word,relevance), sorted by relevance.\n",
    "\"\"\"\n",
    "def compute_word_relevances(documents, max_df=0.8, min_df=1, language='en'):\n",
    "    \n",
    "    texts = [process_text(doc.text, language=language) for doc in documents]\n",
    "    vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df)           \n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "       \n",
    "    for tfidf_vector,doc in zip(tfidf_matrix,documents):\n",
    "        \n",
    "        tfidf_vector = tfidf_vector.toarray().flatten()\n",
    "        sorted_indices = np.argsort(tfidf_vector, axis=None)  \n",
    "        word_relevances = [(feature_names[index], round(tfidf_vector[index],2)) \n",
    "                           for index in reversed(sorted_indices) \n",
    "                           if (not any(char.isdigit() for char in feature_names[index])) and (tfidf_vector[index]>0)]\n",
    "        doc._.word_relevances = word_relevances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Estimates the relevance of a sentence for a document that contains it. The relevance is computed as the average of the relevance scores of its words, stored in the extended attribute called \"word_relevances\". For this reason, this function must be called only after \"compute_word_relevances\". The parameter \"perc_relevant_words\" regulates the percentage of document words (ranked by relevance) that should be included in the computation.\n",
    "\"\"\"\n",
    "def compute_sentence_relevance(sentence, doc, perc_relevant_words=1.0):\n",
    "    \n",
    "    top_n = int(perc_relevant_words * len(doc._.word_relevances))\n",
    "    relevance = 0\n",
    "    n = 0\n",
    "    for w,s in doc._.word_relevances[:top_n]:\n",
    "        if w in sentence.text:\n",
    "            n += 1\n",
    "            relevance += s\n",
    "    \n",
    "    if n > 0:\n",
    "        relevance /= n\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build a custom vector representation for a given spacy document. Stopwords, punctuation, spaces and number characters are always removed. The parameter \"perc_relevant_words\" regulates the percentage of document words (ranked by relevance) that should be included in the computation of the vector. The final vector is obtained as the average of the remaining words. If \"scale_by_tfidf\" is True, then each word-embedding is weighted by its relevance inside the document.\n",
    "\n",
    "The custom vector is stored in the extended attribute of the doc called \"custom_vector\".\n",
    "\"\"\"\n",
    "\n",
    "def refine_doc_vector(doc, perc_relevant_words=1.0, scale_by_tfidf=False, language='en'):\n",
    "    \n",
    "    #if language == 'ca':\n",
    "    #    assert catalan_word_embeddings != None\n",
    "        \n",
    "    top_n = int(perc_relevant_words * len(doc._.word_relevances))\n",
    "    relevant_words = [w for w,s in doc._.word_relevances][:top_n]\n",
    "    relevance_scores = [s for w,s in doc._.word_relevances][:top_n]\n",
    "    \n",
    "    new_vector = np.zeros(768)\n",
    "    normalizer = 0\n",
    "    for token in doc:\n",
    "        if not (token.is_stop or token.is_punct or token.is_digit or token.is_space or token.is_bracket or any(char.isdigit() \n",
    "                                                                                                               for char in token.text)):\n",
    "            if token.text.lower() in relevant_words:\n",
    "                if scale_by_tfidf:\n",
    "                    factor = relevance_scores[relevant_words.index(token.text.lower())]\n",
    "                else:\n",
    "                    factor = 1\n",
    "                #if language == 'en':\n",
    "                #    vec = token.vector\n",
    "                #else:\n",
    "                #    vec = ft.get_word_vector(token.text)\n",
    "                new_vector += (token.vector * factor)\n",
    "                normalizer += factor\n",
    "            \n",
    "    if normalizer > 0:\n",
    "        new_vector /= normalizer\n",
    "    doc._.custom_vector = new_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applies the function \"refine_doc_vector\" to a sentence, treating it exactly as if it were a spacy document. Before calling that function, the word relevances associated to the document are trasferred to the sentence.\n",
    "\"\"\"\n",
    "def refine_sentence_vector(sentence, doc, perc_relevant_words=1.0, scale_by_tfidf=False, language='en'):\n",
    "    sentence._.word_relevances = doc._.word_relevances\n",
    "    return refine_doc_vector(sentence, perc_relevant_words=perc_relevant_words, scale_by_tfidf=scale_by_tfidf,\n",
    "                             language=language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a document, computes its relevance for each of the sustainable development goals. The relevance is computed as the cosine similarity between the vectors (either default or custom).\n",
    "\n",
    "The returned list is not sorted by relevance.\n",
    "\n",
    "ATTENTION:\n",
    "S’han exclòs l’ODS 8 (treball digne i creixement econòmic), l’ODS 9 (indústria, innovació i\n",
    "infraestructures) i l’ODS 17 (aliança pels objectius), perquè no tindrien cap efecte en el filtratge\n",
    "de dades, atès que aquests tres objectius són inherents a tots els projectes col·laboratius\n",
    "d’R+D+I que integra la Plataforma.\n",
    "\"\"\"\n",
    "def compute_goal_scores(project_doc, goal_docs, similarity='custom'):\n",
    "    assert similarity in ('default', 'custom')\n",
    "    \n",
    "    scores = []\n",
    "    for gdoc in goal_docs:\n",
    "        if similarity == 'default':\n",
    "            s = project_doc.similarity(gdoc)\n",
    "        elif similarity == 'custom':\n",
    "            s = project_doc._.custom_similarity(gdoc)\n",
    "        scores.append(s)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_project_and_goal_docs(projects_df, goals_df, use_goal_descriptions=True, use_goal_facts=True, \n",
    "                                   use_goal_targets=True, max_document_frequency=0.3, min_document_frequency=2, \n",
    "                                   perc_relevant_words=1.0, scale_by_tfidf=True, language='en'):\n",
    "    assert language in ('en','ca')\n",
    "    \n",
    "    if language == 'ca':\n",
    "        nlp = spacy.load('ca_fasttext_wiki_lg')\n",
    "        #catalan_word_embeddings = fasttext.load_model('models/word-embeddings/cc.ca.300.bin')\n",
    "    else:\n",
    "        # load the spacy model with BERT embeddings\n",
    "        nlp = en_trf_bertbaseuncased_lg.load()\n",
    "        #catalan_word_embeddings = None\n",
    "        \n",
    "    set_spacy_extensions()\n",
    "    \n",
    "    # read the project data.\n",
    "    projects_df['sdgName'] = [g.split(',') if type(g) == type('str') else [] for g in projects_df['sdgName']]\n",
    "    project_docs = []\n",
    "    project_attributes = [{'projectId':row['projectId'], 'projectTitle':row['projectTitle'], 'sdgName':row['sdgName']} \n",
    "                              for _,row in projects_df.iterrows()]\n",
    "    print('\\nCreating the project docs...')\n",
    "    h = 0\n",
    "    for doc, attr in nlp.pipe(zip(projects_df['projectAbstract'], project_attributes), as_tuples=True):\n",
    "        if h % 100 == 0:\n",
    "            print(100 * (h/ len(projects_df['projectAbstract'])), '%')\n",
    "        h += 1\n",
    "        doc._.project_id = attr['projectId']\n",
    "        doc._.project_title = attr['projectTitle']\n",
    "        doc._.goal_labels = [clean_labels_mapping[l] for l in attr['sdgName']]\n",
    "        project_docs.append(doc)\n",
    "            \n",
    "    # read the sdg data.    \n",
    "    goal_texts = get_goal_texts(goals_df, description=use_goal_descriptions, facts=use_goal_facts, targets=use_goal_targets)\n",
    "    goal_docs = []\n",
    "    for doc, goal_label in nlp.pipe(zip(goal_texts, goals_df['goal_label']), as_tuples=True):\n",
    "        if str(goal_label) in clean_labels_mapping:\n",
    "            doc._.goal_labels = clean_labels_mapping[str(goal_label)] \n",
    "            goal_docs.append(doc)\n",
    "        \n",
    "    print('\\nComputing word relevances...')\n",
    "    compute_word_relevances(goal_docs+project_docs, max_df=max_document_frequency, min_df=min_document_frequency,\n",
    "                            language=language)\n",
    "    print('Refining vectors...')\n",
    "    for doc in goal_docs+project_docs:\n",
    "        refine_doc_vector(doc, perc_relevant_words=perc_relevant_words, scale_by_tfidf=scale_by_tfidf, \n",
    "                          language=language)\n",
    "    print('Done')\n",
    "    \n",
    "    labeled_project_docs = [pdoc for pdoc in project_docs if len(pdoc._.goal_labels) > 0]\n",
    "    unlabeled_project_docs = [pdoc for pdoc in project_docs if len(pdoc._.goal_labels) == 0]\n",
    "    \n",
    "    return labeled_project_docs, unlabeled_project_docs, goal_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
