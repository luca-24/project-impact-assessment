{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import coverage_error, label_ranking_average_precision_score, label_ranking_loss\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_labels_by_threshold(scores, threshold=0.3):\n",
    "    \n",
    "    ranked_labels = sorted([(l,s) for l,s in enumerate(scores)], reverse=True, key=lambda x:x[-1])\n",
    "    selected_labels = [l for l,s in ranked_labels if s >= threshold]\n",
    "    return selected_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_labels_by_k(scores, k=3):\n",
    "    \n",
    "    ranked_labels = sorted([(l,s) for l,s in enumerate(scores)], reverse=True, key=lambda x:x[-1])\n",
    "    selected_labels = [l for l,s in ranked_labels][:k]\n",
    "    return selected_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_labels_by_dcg(scores, min_diff=0.1):\n",
    "    \n",
    "    ranked_labels = sorted([(l,s) for l,s in enumerate(scores)], reverse=True, key=lambda x:x[-1])\n",
    "    \n",
    "    prev_dcg = 0\n",
    "    pos = len(ranked_labels)\n",
    "    for j,(l,s) in enumerate(ranked_labels):\n",
    "        new_dcg = prev_dcg + ((pow(2,s)-1) / math.log2(j + 2))\n",
    "        if new_dcg - prev_dcg < min_diff:\n",
    "            pos = j\n",
    "            break\n",
    "        prev_dcg = new_dcg\n",
    "    \n",
    "    selected_labels = [l for l,s in ranked_labels][:j]\n",
    "    return selected_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _precision_single(y_true, y_pred):\n",
    "    \n",
    "    precision = 0\n",
    "    for l in y_pred:\n",
    "        if l in y_true:\n",
    "            precision += 1\n",
    "    \n",
    "    if len(y_pred) > 0:\n",
    "        precision /= len(y_pred)\n",
    "    else:\n",
    "        precision = np.nan\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recall_single(y_true, y_pred):\n",
    "    \n",
    "    recall = 0\n",
    "    for l in y_true:\n",
    "        if l in y_pred:\n",
    "            recall += 1\n",
    "    \n",
    "    if len(y_true) > 0:\n",
    "        recall /= len(y_true)\n",
    "    else:\n",
    "        recall = np.nan\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_label_frequencies(y_true):\n",
    "    \n",
    "    freqs = [0 for i in range(len(y_true[0]))]\n",
    "    for yt in y_true:\n",
    "        for i,l in enumerate(yt):\n",
    "            freqs[i] += l\n",
    "    \n",
    "    freqs = [f/len(y_true) for f in freqs]\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_bias(metric_array_x, metric_array_y, print_plot=True, xlabel='', ylabel=''):\n",
    "    \n",
    "    labels = [i for i,x in enumerate(metric_array_x)]\n",
    "    sorted_arrays = sorted(list(zip(metric_array_x, metric_array_y, labels)))\n",
    "    metric_array_x = [x for x,y,l in sorted_arrays]\n",
    "    metric_array_y = [y for x,y,l in sorted_arrays]\n",
    "    labels = [l for x,y,l in sorted_arrays]\n",
    "    \n",
    "    z = np.polyfit(metric_array_x, metric_array_y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    \n",
    "    angle = math.atan(z[0])\n",
    "    intercept_on_y = p(0)\n",
    "    imbalance_bias_coefficient, p_value = stats.pearsonr(metric_array_x, metric_array_y)\n",
    "    \n",
    "    if print_plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        title = ('Angle with x axis: ' + str(round(math.degrees(angle),2)) + 'Â°' +\n",
    "                 '\\nPearson Correlation: ' + str(round(imbalance_bias_coefficient,2)) +\n",
    "                 '\\nIntercept on y axis: ' + str(round(intercept_on_y,2)))\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_xlim([0,1])\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.scatter(metric_array_x, metric_array_y)\n",
    "        for x, y, l in zip(metric_array_x, metric_array_y, labels):\n",
    "            ax.annotate(l, (x, y))\n",
    "        ax.plot(metric_array_x, p(metric_array_x),\"r--\")\n",
    "        plt.show()\n",
    "    \n",
    "    return imbalance_bias_coefficient, intercept_on_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_imbalance_bias_metrics(metrics, training_frequencies, print_plots=True):\n",
    "    \n",
    "    bias_metrics = {}\n",
    "    metrics_array = []\n",
    "    metrics.pop('average')\n",
    "    for l in sorted(metrics.keys()):\n",
    "        metrics_array.append((metrics[l]['true_frequency'], \n",
    "                              metrics[l]['R_precision'],\n",
    "                              metrics[l]['precision'],\n",
    "                              metrics[l]['recall'],\n",
    "                              metrics[l]['fscore']))\n",
    "    \n",
    "    frequencies = [x[0] for x in metrics_array]\n",
    "    r_precisions = [x[1] for x in metrics_array]\n",
    "    precisions = [x[2] for x in metrics_array]\n",
    "    recalls = [x[3] for x in metrics_array]\n",
    "    fscores = [x[4] for x in metrics_array]\n",
    "       \n",
    "    bias_metrics['R_precision_IBC'], bias_metrics['R_precision_at_null_frequency'] = _get_bias(training_frequencies, r_precisions,\n",
    "                                                                                               print_plot=print_plots,\n",
    "                                                                                               xlabel='frequency in training', \n",
    "                                                                                               ylabel='R-Precision')\n",
    "    bias_metrics['precision_IBC'], bias_metrics['precision_at_null_frequency'] = _get_bias(training_frequencies, precisions,\n",
    "                                                                                           print_plot=print_plots,\n",
    "                                                                                           xlabel='frequency in training', \n",
    "                                                                                           ylabel='Precision')\n",
    "    bias_metrics['recall_IBC'], bias_metrics['recall_at_null_frequency'] = _get_bias(training_frequencies, recalls,\n",
    "                                                                                     print_plot=print_plots,\n",
    "                                                                                     xlabel='frequency in training', \n",
    "                                                                                     ylabel='Recall')\n",
    "    bias_metrics['fscore_IBC'], bias_metrics['fscore_at_null_frequency'] = _get_bias(training_frequencies, fscores,\n",
    "                                                                                     print_plot=print_plots,\n",
    "                                                                                     xlabel='frequency in training', \n",
    "                                                                                     ylabel='F-score')\n",
    "    return bias_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binary_classification_metrics_per_class(y_true, y_score, included_labels=range(14), \n",
    "                                                    label_selection_method='dcg', dcg_min_diff=0.1, threshold=0.3, k=3):\n",
    "    \n",
    "    assert label_selection_method in ('dcg', 'threshold', 'k')\n",
    "    if label_selection_method == 'dcg':\n",
    "        assert dcg_min_diff\n",
    "    elif label_selection_method == 'threshold':\n",
    "        assert threshold\n",
    "    elif label_selection_method == 'k':\n",
    "        assert k\n",
    "        \n",
    "    metrics = {l:{} for l in included_labels}\n",
    "    \n",
    "    for binary_label in included_labels:\n",
    "        true_labels_binary = []\n",
    "        R_predicted_labels_binary = []\n",
    "        predicted_labels_binary = []\n",
    "        \n",
    "        for i in range(len(y_true)):\n",
    "            yt = y_true[i]\n",
    "            ys = y_score[i]\n",
    "            \n",
    "            all_true_labels = [l for l,v in enumerate(yt) if v]\n",
    "            binary_true_label = 1 if binary_label in all_true_labels else 0\n",
    "            \n",
    "            all_R_predicted_labels = select_labels_by_k(ys, k=len(all_true_labels))\n",
    "            binary_R_predicted_label = 1 if binary_label in all_R_predicted_labels else 0\n",
    "                            \n",
    "            if label_selection_method == 'dcg':\n",
    "                all_predicted_labels = select_labels_by_dcg(ys, min_diff=dcg_min_diff)\n",
    "            elif label_selection_method == 'threshold':\n",
    "                all_predicted_labels = select_labels_by_threshold(ys, threshold=threshold)\n",
    "            elif label_selection_method == 'k':\n",
    "                all_predicted_labels = select_labels_by_k(ys, k=k)\n",
    "            binary_predicted_label = 1 if binary_label in all_predicted_labels else 0\n",
    "            \n",
    "            true_labels_binary.append(binary_true_label)\n",
    "            R_predicted_labels_binary.append(binary_R_predicted_label)\n",
    "            predicted_labels_binary.append(binary_predicted_label)\n",
    "        \n",
    "        metrics[binary_label]['R_precision'] = precision_score(true_labels_binary, R_predicted_labels_binary)\n",
    "        metrics[binary_label]['precision'] = precision_score(true_labels_binary, predicted_labels_binary)\n",
    "        metrics[binary_label]['recall'] = recall_score(true_labels_binary, predicted_labels_binary)\n",
    "        metrics[binary_label]['fscore'] = f1_score(true_labels_binary, predicted_labels_binary)\n",
    "        metrics[binary_label]['true_frequency'] = len([x for x in true_labels_binary if x == 1]) / len(true_labels_binary)\n",
    "        metrics[binary_label]['R_predicted_frequency'] = len([x for x in R_predicted_labels_binary if x == 1]) / len(true_labels_binary)\n",
    "        metrics[binary_label]['predicted_frequency'] = len([x for x in predicted_labels_binary if x == 1]) / len(true_labels_binary)\n",
    "        \n",
    "    avg_R_precision = 0\n",
    "    avg_precision = 0\n",
    "    avg_recall = 0\n",
    "    avg_fscore = 0\n",
    "    total_true_frequency = 0\n",
    "    for l in metrics:\n",
    "        avg_R_precision += (metrics[l]['R_precision'] * metrics[l]['true_frequency'])\n",
    "        avg_precision += (metrics[l]['precision'] * metrics[l]['true_frequency'])\n",
    "        avg_recall += (metrics[l]['recall'] * metrics[l]['true_frequency'])\n",
    "        avg_fscore += (metrics[l]['fscore'] * metrics[l]['true_frequency'])\n",
    "        total_true_frequency += metrics[l]['true_frequency']\n",
    "    \n",
    "    metrics['average'] = {'R_precision': avg_R_precision / total_true_frequency,\n",
    "                          'precision': avg_precision / total_true_frequency,\n",
    "                          'recall': avg_recall / total_true_frequency,\n",
    "                          'fscore': avg_fscore / total_true_frequency}\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(y_true, y_score, label_selection_method='dcg', dcg_min_diff=0.1, threshold=0.3, k=3):\n",
    "    assert label_selection_method in ('dcg', 'threshold', 'k')\n",
    "    if label_selection_method == 'dcg':\n",
    "        assert dcg_min_diff\n",
    "    elif label_selection_method == 'threshold':\n",
    "        assert threshold\n",
    "    elif label_selection_method == 'k':\n",
    "        assert k\n",
    "    \n",
    "    avg_R_precision = 0\n",
    "    avg_precision = 0\n",
    "    avg_recall = 0\n",
    "    avg_fscore = 0\n",
    "    avg_n_true_labels = 0\n",
    "    avg_n_predicted_labels = 0\n",
    "    \n",
    "    \n",
    "    R_precision_normalizer = 0\n",
    "    precision_normalizer = 0\n",
    "    recall_normalizer = 0\n",
    "    fscore_normalizer = 0\n",
    "    for yt, ys in zip(y_true, y_score):\n",
    "        #print('\\n\\ny_true sparse:', yt)\n",
    "        #print('y_score sparse:', ys)\n",
    "        true_labels = [l for l,v in enumerate(yt) if v]\n",
    "        avg_n_true_labels += len(true_labels)\n",
    "        \n",
    "        R_predicted_labels = select_labels_by_k(ys, k=len(true_labels))\n",
    "        R_p = _precision_single(true_labels, R_predicted_labels)\n",
    "        if not np.isnan(R_p):\n",
    "            #print('\\nprecision:', p)\n",
    "            avg_R_precision += R_p\n",
    "            R_precision_normalizer += 1\n",
    "        \n",
    "        if label_selection_method == 'dcg':\n",
    "            predicted_labels = select_labels_by_dcg(ys, min_diff=dcg_min_diff)\n",
    "        elif label_selection_method == 'threshold':\n",
    "            predicted_labels = select_labels_by_threshold(ys, threshold=threshold)\n",
    "        elif label_selection_method == 'k':\n",
    "            predicted_labels = select_labels_by_k(ys, k=k)\n",
    "            \n",
    "        avg_n_predicted_labels += len(predicted_labels)\n",
    "\n",
    "        p = _precision_single(true_labels, predicted_labels)\n",
    "        if not np.isnan(p):\n",
    "            avg_precision += p\n",
    "            precision_normalizer += 1\n",
    "        \n",
    "        r = _recall_single(true_labels, predicted_labels)\n",
    "        if not np.isnan(r):\n",
    "            avg_recall += r\n",
    "            recall_normalizer += 1\n",
    "            \n",
    "        if (not np.isnan(p)) and (not np.isnan(r)):\n",
    "            if (p+r) > 0:\n",
    "                f = (2 * p * r) / (p + r)\n",
    "            else:\n",
    "                f = 0.0\n",
    "            avg_fscore += f\n",
    "            fscore_normalizer += 1\n",
    "    \n",
    "    if R_precision_normalizer > 0:\n",
    "        avg_R_precision /= R_precision_normalizer\n",
    "    if precision_normalizer > 0:\n",
    "        avg_precision /= precision_normalizer\n",
    "    if recall_normalizer > 0:\n",
    "        avg_recall /= recall_normalizer\n",
    "    if fscore_normalizer > 0:\n",
    "        avg_fscore /= fscore_normalizer\n",
    "    avg_n_true_labels /= len(y_true)\n",
    "    avg_n_predicted_labels /= len(y_true)\n",
    "    \n",
    "    metrics = {}\n",
    "    metrics['avg_R_precision'] = avg_R_precision\n",
    "    metrics['avg_precision'] = avg_precision\n",
    "    metrics['avg_recall'] = avg_recall\n",
    "    metrics['avg_fscore'] = avg_fscore\n",
    "    metrics['avg_n_true_labels'] = avg_n_true_labels\n",
    "    metrics['avg_n_predicted_labels'] = avg_n_predicted_labels\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_coverage_error(y_true, y_score):\n",
    "    cov_err = coverage_error(y_true, y_score)\n",
    "    best_possible_cov_err = np.average([sum(yt) for yt in y_true])\n",
    "    worst_possible_cov_err = len(y_true[0])\n",
    "    \n",
    "    normalized_cov_err = (cov_err - best_possible_cov_err) / (worst_possible_cov_err - best_possible_cov_err)\n",
    "    \n",
    "    return normalized_cov_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computes the average exposure assigned by the ranking to the ground truth labels. Works with\n",
    "one single array of y_true and y_score, in a non-aggregated way.\n",
    "Follows the definition of exposure by Singh et al., 2018.\n",
    "\"\"\"\n",
    "def _avg_exposure_single(y_true, y_score, normalize=True):\n",
    "    \n",
    "    true_labels = [l for l,v in enumerate(y_true) if v]\n",
    "    ranked_labels = sorted([(l,s) for l,s in enumerate(y_score)], reverse=True, key=lambda x:x[-1])\n",
    "\n",
    "    avg_exp = 0\n",
    "    for j,(l,s) in enumerate(ranked_labels):\n",
    "        if l in true_labels:\n",
    "            avg_exp += 1 / math.log2(2+j)\n",
    "    \n",
    "    if normalize:\n",
    "        best_possible_exp = sum([(1 / math.log2(2+j)) for j in range(len(true_labels))])\n",
    "        worst_possible_exp = sum([(1 / math.log2(2+j)) for j in range(len(ranked_labels)-1,len(ranked_labels)-len(true_labels)-1,-1)])\n",
    "\n",
    "        avg_exp = (avg_exp - worst_possible_exp) / (best_possible_exp-worst_possible_exp)\n",
    "\n",
    "    return avg_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_exposure_aggregate(y_true, y_score):\n",
    "    \n",
    "    avg_exp_aggr = 0\n",
    "    for yt, ys in zip(y_true, y_score):\n",
    "        avg_exp_aggr += _avg_exposure_single(yt, ys)\n",
    "    avg_exp_aggr /= len(y_true)\n",
    "    return avg_exp_aggr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranking_metrics(y_true, y_score):\n",
    "    metrics = {}\n",
    "    metrics['lraps'] = label_ranking_average_precision_score(y_true, y_score)\n",
    "    metrics['lrl'] = label_ranking_loss(y_true, y_score)\n",
    "    metrics['cov_err'] = normalized_coverage_error(y_true, y_score)\n",
    "    metrics['avg_exp'] = avg_exposure_aggregate(y_true, y_score)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics):\n",
    "    \n",
    "    metrics_full_names = {'lraps': 'Label Ranking Average Precision',\n",
    "                          'lrl': 'Label Ranking Loss',\n",
    "                          'cov_err': 'Coverage Error',\n",
    "                          'avg_exp': 'Average Exposure',\n",
    "                          'avg_R_precision': 'Average R-Precision',\n",
    "                          'avg_precision': 'Average Precision',\n",
    "                          'avg_recall': 'Average Recall',\n",
    "                          'avg_fscore': 'Average F-score',\n",
    "                          'avg_n_true_labels': 'Average number of true labels',\n",
    "                          'avg_n_predicted_labels': 'Average number of predicted labels',\n",
    "                          'R_precision': 'R-Precision',\n",
    "                          'precision': 'Precision',\n",
    "                          'recall': 'Recall',\n",
    "                          'fscore': 'F-score',\n",
    "                          'true_frequency': 'Frequency in the ground truth (%)',\n",
    "                          'R_predicted_frequency': 'Frequency in the predictions @ R',\n",
    "                          'predicted_frequency': 'Frequency in the predictions',\n",
    "                          'R_precision_IBC': 'R-Precision Imbalance Bias Coefficient',\n",
    "                          'precision_IBC': 'Precision Imbalance Bias Coefficient',\n",
    "                          'recall_IBC': 'Recall Imbalance Bias Coefficient',\n",
    "                          'fscore_IBC': 'F-score Imbalance Bias Coefficient',\n",
    "                          'R_precision_at_null_frequency' : 'Asymptotic R-Precision At Null Frequency',\n",
    "                          'precision_at_null_frequency' : 'Asymptotic Precision At Null Frequency',\n",
    "                          'recall_at_null_frequency' : 'Asymptotic Recall At Null Frequency',\n",
    "                          'fscore_at_null_frequency' : 'Asymptotic F-score At Null Frequency'}\n",
    "    \n",
    "    for m in metrics:\n",
    "        print(metrics_full_names[m], ':', round(metrics[m],2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
