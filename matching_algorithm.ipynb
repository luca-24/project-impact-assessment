{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching algorithm\n",
    "\n",
    "This scripts implements function to:\n",
    "- represent textual data of projects and sdg in a vector space, by means of word-embeddings;\n",
    "- assign a list of relevance scores to projects, one for each goal, using similarity metrics;\n",
    "- evaluate the accuracy of the matching algorithm by means of standard metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_docs = True\n",
    "dump_docs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, pickle, random\n",
    "import numpy as np\n",
    "from sklearn.metrics import coverage_error, label_ranking_average_precision_score, label_ranking_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nlp_functions, explanation, utils, evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the relevance scores to sdgs for each spacy document in a given list, using either the default or the custom similarity. If \"only_projects_with_labels\" is True, then the projects that in the original dataset have 0 true labels are ignored.\n",
    "\n",
    "Returns three lists, where at each position there are the true labels, the estimated scores (not sorted) and the id of the project they refer to, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_matching(project_docs, goal_docs, similarity='custom'):\n",
    "    y_true = []\n",
    "    y_score = []\n",
    "    for pdoc in project_docs:\n",
    "        true_labels = [1 if str(i) in pdoc._.goal_labels else 0 for i in range(14)]\n",
    "        label_scores = nlp_functions.compute_goal_scores(pdoc, goal_docs, similarity=similarity)\n",
    "        if any(np.isnan(ls) for ls in label_scores):\n",
    "            continue\n",
    "        pdoc._.predicted_goal_scores = sorted([(i+1,s) for i,s in enumerate(label_scores)], reverse=True, key=lambda x:x[-1])\n",
    "        y_true.append(true_labels)\n",
    "        y_score.append(label_scores)\n",
    "    return y_true, y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_spacy_extensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_docs:\n",
    "    with open('data/pickles/project_docs_labeled_optimized.pkl', 'rb') as f:\n",
    "        project_docs = pickle.load(f)\n",
    "    with open('data/pickles/goal_docs_optimized.pkl', 'rb') as f:\n",
    "        goal_docs = pickle.load(f)\n",
    "else:\n",
    "    projects_df = pandas.read_csv('data/ris3-mcat-projects-cleaned.csv', sep='\\t')\n",
    "    goals_df = pandas.read_excel('data/un-goals.xlsx')\n",
    "    project_docs, goal_docs = nlp_functions.generate_project_and_goal_docs(projects_df, goals_df, select_projects='labeled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dump_docs:\n",
    "    with open('data/pickles/project_docs_labeled_optimized.pkl', 'wb') as f:\n",
    "        pickle.dump(project_docs, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('data/pickles/goal_docs_optimized.pkl', 'wb') as f:\n",
    "        pickle.dump(goal_docs, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "test_project_docs = random.sample(project_docs, 200)\n",
    "training_project_docs = [pdoc for pdoc in project_docs if pdoc not in test_project_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the matching algorithm.\n",
    "\n",
    "For reference about the metrics:\n",
    "- Label ranking average precision: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html#sklearn.metrics.label_ranking_average_precision_score\n",
    "- Label ranking loss: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_loss.html#sklearn.metrics.label_ranking_loss\n",
    "- Coverage error: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.coverage_error.html#sklearn.metrics.coverage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_train, y_score_train = apply_matching(training_project_docs, goal_docs, similarity='custom')\n",
    "\n",
    "for y in y_score_train:\n",
    "    evaluation.select_labels_from_scores(y)\n",
    "raise Exception\n",
    "\n",
    "ranking_metrics = evaluation.compute_ranking_metrics(y_true_train, y_score_train)\n",
    "classification_metrics = evaluation.compute_classification_metrics(y_true_train, y_score_train, \n",
    "                                                                   label_selection_method='dcg')\n",
    "\n",
    "print('RESULTS ON TRAINING SET')\n",
    "evaluation.print_metrics(ranking_metrics)\n",
    "evaluation.print_metrics(classification_metrics)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_test, y_score_test = apply_matching(project_docs, goal_docs, similarity='custom')\n",
    "\n",
    "ranking_metrics = evaluation.compute_ranking_metrics(y_true_test, y_score_test)\n",
    "classification_metrics = evaluation.compute_classification_metrics(y_true_test, y_score_test, \n",
    "                                                                   label_selection_k='R')\n",
    "\n",
    "print('RESULTS ON TEST SET')\n",
    "evaluation.print_metrics(ranking_metrics)\n",
    "evaluation.print_metrics(classification_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.visualize_output(project_docs, goal_docs, percentile_highlighted_words=75, use_colors=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
